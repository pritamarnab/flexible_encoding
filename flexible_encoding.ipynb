{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "233049f1-8482-4175-bf9f-f532d179a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import csv\n",
    "import string\n",
    "from scipy.io import savemat\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from functools import reduce \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "import torch.utils.data as data\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d625ec-6a15-495a-9d55-081770143296",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=50\n",
    "num_words=500\n",
    "emb_dim=50\n",
    "num_lags=3\n",
    "EPOCHS = 5\n",
    "\n",
    "\n",
    "electrode=5  #which electrode are we considering\n",
    "taking_words=False  # are we taking the actual words or the lags around the last word\n",
    "lag_number=3 # how many previous lags/words to consider\n",
    "lags=[0,-50,50] # only valid if taking word is false and should equal to the lag_number\n",
    "train_num=3500  \n",
    "subject=798\n",
    "min_num_words=5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8613c7eb-c2e8-4088-b031-e8f39d33c1bb",
   "metadata": {},
   "source": [
    "## Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14e2375-a408-4a91-a1e3-f0d6499a64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings=np.random.randn(num_words, 1,emb_dim)\n",
    "# elec_data=np.random.randn(num_words, 1,num_lags)\n",
    "\n",
    "# X_train=torch.from_numpy(embeddings[:400,:,:])\n",
    "# y_train=torch.from_numpy(elec_data[:400,:,:])\n",
    "\n",
    "# X_test=torch.from_numpy(embeddings[400:,:,:])\n",
    "# y_test=torch.from_numpy(elec_data[400:,:,:])\n",
    "\n",
    "# trainset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# testset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "# testloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55d23e2a-988a-479e-be19-4196f7847183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(file_path):\n",
    "    \n",
    "    pickle_file = open(file_path, \"rb\")\n",
    "    objects = []\n",
    "\n",
    "    i=0\n",
    "\n",
    "    while True:\n",
    "        # print('i',i)\n",
    "        try:\n",
    "\n",
    "            objects.append(pickle.load(pickle_file))\n",
    "\n",
    "        except EOFError:\n",
    "\n",
    "            break\n",
    "\n",
    "    pickle_file.close()\n",
    "\n",
    "    a=objects[0]\n",
    "    \n",
    "    return a\n",
    "\n",
    "def load_label(filepath):\n",
    "\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        full_labels = pickle.load(f)\n",
    "        #labels_df = pd.DataFrame(full_labels[\"labels\"])\n",
    "        labels_df = pd.DataFrame(full_labels)\n",
    "\n",
    "    # labels_df[\"audio_onset\"] = ((labels_df.onset + 3000) / 512)\n",
    "    # labels_df[\"audio_offset\"] = ((labels_df.offset + 3000) / 512)\n",
    "\n",
    "    # labels_df = labels_df.dropna(subset=[\"audio_onset\", \"audio_offset\"])\n",
    "    \n",
    "\n",
    "    return labels_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eee3c50b-f428-4729-b503-9f2d3ecc7b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elec_id(subject):\n",
    "\n",
    "    path='/projects/HASSON/247/plotting/sig-elecs/20230510-tfs-sig-file/'\n",
    "    \n",
    "    sig_file=path+'tfs-sig-file-glove-'+ str(subject)+'-'+'comp'+'.csv'\n",
    "    \n",
    "    df_sig=pd.read_csv(sig_file)\n",
    "    \n",
    "    elecs=df_sig.electrode.values\n",
    "    \n",
    "    \n",
    "    \n",
    "    path='/scratch/gpfs/kw1166/247/247-pickling/results/tfs/'+str(subject)+'/pickles/'+str(subject)+'_electrode_names.pkl'\n",
    "    \n",
    "    pickle_file = open(path, \"rb\")\n",
    "    objects = []\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    while True:\n",
    "        # print('i',i)\n",
    "        try:\n",
    "    \n",
    "            objects.append(pickle.load(pickle_file))\n",
    "    \n",
    "        except EOFError:\n",
    "    \n",
    "            break\n",
    "    \n",
    "    pickle_file.close()\n",
    "    \n",
    "    a=objects[0]\n",
    "    \n",
    "    df_name_match=pd.DataFrame(a)\n",
    "    \n",
    "    elec_id=[]\n",
    "    for elec in elecs:\n",
    "        elec_id.extend(df_name_match[df_name_match.electrode_name==elec].electrode_id.values)\n",
    "                      \n",
    "    return elec_id\n",
    "\n",
    "def get_elec_data(subject, df, conv_name, all_onsets, lags, elec_id, lag_number,taking_words=True):\n",
    "\n",
    "    path='/projects/HASSON/247/data/conversations-car/'+str(subject)+'/'\n",
    "    # conv_name=df[df.conversation_id==conv_id].conversation_name.values[0]\n",
    "    # q1=glob.glob(\"*_comp_Y.npy\")\n",
    "    # q1=glob.glob(\"NY625_*\") ## all the conversation\n",
    "    # # path_elec=path+'/'+q1[0]+'/preprocessed/'\n",
    "    \n",
    "    elec_num=len(elec_id)\n",
    "    \n",
    "    # Y_data= np.zeros((len(lags),len(onsets),elec_num)) \n",
    "\n",
    "    Y_data= np.zeros((elec_num, lag_number)) \n",
    "\n",
    "    ecogs=[]\n",
    "    for k1 in elec_id:\n",
    "        filename=path+'/'+conv_name+'/preprocessed/'+conv_name+'_electrode_preprocess_file_'+str(k1)+'.mat'\n",
    "\n",
    "        if subject==798:\n",
    "            filename=path+'/'+conv_name+'/preprocessed_allElec/'+conv_name+'_electrode_preprocess_file_'+str(k1)+'.mat'\n",
    "        \n",
    "        e=loadmat(filename)['p1st'].squeeze().astype(np.float32)\n",
    "        ecogs.append(e)\n",
    "    \n",
    "    ecogs = np.asarray(ecogs).T\n",
    "    t=len(ecogs[:,0])\n",
    "    \n",
    "    window_size=200\n",
    "    half_window = round((window_size / 1000) * 512 / 2)\n",
    "\n",
    "    if taking_words:\n",
    "\n",
    "        onsets=all_onsets[-lag_number:-1]\n",
    "        onsets=np.append(onsets,all_onsets[-1])\n",
    "    else:\n",
    "\n",
    "        for i in lags:\n",
    "\n",
    "            lag_amount = int(lags[i]/ 1000 * 512)\n",
    "\n",
    "            onsets.append(np.minimum(\n",
    "                t - half_window - 1,\n",
    "                np.maximum(half_window + 1,\n",
    "                            np.round(all_onsets[-1], 0, all_onsets[-1]) + lag_amount)))\n",
    "\n",
    "    \n",
    "    \n",
    "    index_onsets=np.asarray(onsets)\n",
    "    print(index_onsets)\n",
    "\n",
    "    print(np.shape(index_onsets))\n",
    "    \n",
    "    # Y= np.zeros((len(onsets), np.shape(ecogs)[1]))\n",
    "    \n",
    "        \n",
    "    # for ii in range(len(lags)):\n",
    "   \n",
    "\n",
    "    for k in range(np.shape(ecogs)[1]):\n",
    "\n",
    "        Y1 = np.zeros((len(onsets), 2 * half_window + 1))\n",
    "        brain_signal=ecogs[:,k]\n",
    "        \n",
    "        # subtracting 1 from starts to account for 0-indexing\n",
    "        starts = index_onsets - half_window - 1\n",
    "        stops = index_onsets + half_window\n",
    "        \n",
    "        for i, (start, stop) in enumerate(zip(starts, stops)):\n",
    "            start=int(start)\n",
    "            stop=int(stop)\n",
    "            Y1[i, :] = brain_signal[start:stop].reshape(-1)\n",
    "            \n",
    "                            \n",
    "        #if subject==717:\n",
    "           \n",
    "        Y_data[k,:] = np.mean(Y1, axis=-1)\n",
    "\n",
    "    return Y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a14f6007-2659-434c-b64b-8054c5a78c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elec_data(subject, df, ecogs, conv_name, all_onsets, lags, elec_id, lag_number,taking_words=True):\n",
    "\n",
    "    # path='/projects/HASSON/247/data/conversations-car/'+str(subject)+'/'\n",
    "    # # conv_name=df[df.conversation_id==conv_id].conversation_name.values[0]\n",
    "    # # q1=glob.glob(\"*_comp_Y.npy\")\n",
    "    # # q1=glob.glob(\"NY625_*\") ## all the conversation\n",
    "    # # # path_elec=path+'/'+q1[0]+'/preprocessed/'\n",
    "    \n",
    "    \n",
    "    \n",
    "    # # Y_data= np.zeros((len(lags),len(onsets),elec_num)) \n",
    "\n",
    "\n",
    "\n",
    "    # ecogs=[]\n",
    "    # for k1 in elec_id:\n",
    "    #     filename=path+'/'+conv_name+'/preprocessed/'+conv_name+'_electrode_preprocess_file_'+str(k1)+'.mat'\n",
    "\n",
    "    #     if subject==798:\n",
    "    #         filename=path+'/'+conv_name+'/preprocessed_allElec/'+conv_name+'_electrode_preprocess_file_'+str(k1)+'.mat'\n",
    "        \n",
    "    #     e=loadmat(filename)['p1st'].squeeze().astype(np.float32)\n",
    "    #     ecogs.append(e)\n",
    "    \n",
    "    # ecogs = np.asarray(ecogs).T\n",
    "\n",
    "\n",
    "\n",
    "    elec_num=len(elec_id)\n",
    "    t=len(ecogs[:,0])   \n",
    "    window_size=200\n",
    "    half_window = round((window_size / 1000) * 512 / 2)\n",
    "\n",
    "    if taking_words:\n",
    "\n",
    "        Y_data= np.zeros((elec_num, lag_number))    \n",
    "\n",
    "        onsets=all_onsets[-lag_number:-1]\n",
    "        onsets=np.append(onsets,all_onsets[-1])\n",
    "    else:\n",
    "\n",
    "        onsets=[]\n",
    "        Y_data= np.zeros((elec_num, len(lags)))    \n",
    "\n",
    "        for i in lags:\n",
    "\n",
    "            lag_amount = int(i/ 1000 * 512)\n",
    "\n",
    "            onsets.append(np.minimum(\n",
    "                t - half_window - 1,\n",
    "                np.maximum(half_window + 1,\n",
    "                            np.round(all_onsets[-1]) + lag_amount)))\n",
    "\n",
    "    \n",
    "    \n",
    "    index_onsets=np.asarray(onsets)\n",
    "\n",
    "   \n",
    "   \n",
    "    \n",
    "    for k in range(np.shape(ecogs)[1]):\n",
    "\n",
    "        Y1 = np.zeros((len(onsets), 2 * half_window + 1))\n",
    "        brain_signal=ecogs[:,k]\n",
    "        \n",
    "        # subtracting 1 from starts to account for 0-indexing\n",
    "        starts = index_onsets - half_window - 1\n",
    "        stops = index_onsets + half_window\n",
    "\n",
    "        # print(starts)\n",
    "        # print(stops)\n",
    "        \n",
    "        for i, (start, stop) in enumerate(zip(starts, stops)):\n",
    "            start=int(start)\n",
    "            stop=int(stop)\n",
    "            Y1[i, :] = brain_signal[start:stop].reshape(-1)\n",
    "            \n",
    "                            \n",
    "        #if subject==717:\n",
    "           \n",
    "        Y_data[k,:] = np.mean(Y1, axis=-1)\n",
    "\n",
    "    return Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38bef130-f15a-41c4-a8c7-c2862b5e9d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv('/scratch/gpfs/arnab/247_data_read/decoding_df_798_corrected.csv')\n",
    "df=pd.read_csv('/scratch/gpfs/arnab/247_data_read/decoding_df_798_final.csv')\n",
    "\n",
    "data=load_pickle('/scratch/gpfs/arnab/247_data_read/last_word_embeddings.pkl')\n",
    "\n",
    "emb=data['embeddings']\n",
    "a=data['all_onsets']\n",
    "\n",
    "df['embeddings']=emb\n",
    "df['all_onsets']=a\n",
    "\n",
    "df=df.drop_duplicates(subset=['onsets'])\n",
    "\n",
    "df=df[df.num_words>min_num_words]\n",
    "embeddings=df.embeddings.values\n",
    "all_onsets=df.all_onsets.values\n",
    "\n",
    "df=df[df.corrupted==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e4510de-b811-4c25-b8c5-fa584130f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES=30*44100\n",
    "def pad_or_trim(array, length: int = N_SAMPLES, *, axis: int = -1):\n",
    "    \"\"\"\n",
    "    Pad or trim the audio array to N_SAMPLES, as expected by the encoder.\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(array):\n",
    "        if array.shape[axis] > length:\n",
    "            array = array.index_select(\n",
    "                dim=axis, index=torch.arange(length, device=array.device)\n",
    "            )\n",
    "\n",
    "        if array.shape[axis] < length:\n",
    "            pad_widths = [(0, 0)] * array.ndim\n",
    "            pad_widths[axis] = (0, length - array.shape[axis])\n",
    "            array = F.pad(array, [pad for sizes in pad_widths[::-1] for pad in sizes])\n",
    "    else:\n",
    "        if array.shape[axis] > length:\n",
    "            array = array.take(indices=range(length), axis=axis)\n",
    "\n",
    "        if array.shape[axis] < length:\n",
    "            pad_widths = [(0, 0)] * array.ndim\n",
    "            pad_widths[axis] = (0, length - array.shape[axis])\n",
    "            array = np.pad(array, pad_widths)\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe3ef472-842e-4c5d-a431-1482efbc6149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7095,  0.3646,  0.2364,  1.3628],\n",
      "        [ 0.4202, -1.0196,  0.7987,  0.3948],\n",
      "        [-0.7005,  0.2994,  0.8068, -0.0033],\n",
      "        [-1.2695,  0.3986,  0.5787, -1.4536]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2364, -1.0196, -0.7005, -1.4536])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 4)\n",
    "print(a)\n",
    "loss=torch.min(a, -1).values\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "070fd460-0f86-48f7-b3d5-4bb438c7a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_ecog(elec_id, conv_name,subject ):\n",
    "\n",
    "    \n",
    "    elec_id=get_elec_id(subject)\n",
    "    \n",
    "    path='/projects/HASSON/247/data/conversations-car/'+str(subject)+'/'\n",
    "    # conv_name=df[df.conversation_id==conv_id].conversation_name.values[0]\n",
    "    # q1=glob.glob(\"*_comp_Y.npy\")\n",
    "    # q1=glob.glob(\"NY625_*\") ## all the conversation\n",
    "    # # path_elec=path+'/'+q1[0]+'/preprocessed/'\n",
    "    \n",
    "    elec_num=len(elec_id)\n",
    "    \n",
    "    ecogs=[]\n",
    "    for k1 in elec_id:\n",
    "        filename=path+'/'+conv_name+'/preprocessed/'+conv_name+'_electrode_preprocess_file_'+str(k1)+'.mat'\n",
    "        \n",
    "        if subject==798:\n",
    "            filename=path+'/'+conv_name+'/preprocessed_allElec/'+conv_name+'_electrode_preprocess_file_'+str(k1)+'.mat'\n",
    "        \n",
    "        e=loadmat(filename)['p1st'].squeeze().astype(np.float32)\n",
    "        ecogs.append(e)\n",
    "        \n",
    "    ecogs = np.asarray(ecogs).T\n",
    "\n",
    "    return ecogs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9f92475-b5f6-4192-93c7-02eac839b987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "320\n",
      "497\n",
      "889\n",
      "931\n",
      "1804\n",
      "1837\n",
      "1891\n",
      "2860\n",
      "3331\n",
      "3332\n",
      "3820\n",
      "4128\n",
      "4186\n"
     ]
    }
   ],
   "source": [
    "subject=798\n",
    "\n",
    "elec_id=get_elec_id(subject)\n",
    "\n",
    "conv_names=np.unique(df.conversation_name.values)\n",
    "\n",
    "if taking_words:\n",
    "    electrode_data=np.zeros((len(df.conversation_name.values),len(elec_id),lag_number)) \n",
    "else:\n",
    "    electrode_data=np.zeros((len(df.conversation_name.values),len(elec_id),len(lags))) \n",
    "\n",
    "embeddings=[]\n",
    "\n",
    "p=0\n",
    "for conv_name in conv_names:\n",
    "\n",
    "    print(p)\n",
    "\n",
    "    ecogs=all_ecog(elec_id, conv_name,subject)\n",
    "\n",
    "    df2=df[df.conversation_name==conv_name]\n",
    "\n",
    "    emb=df2.embeddings.values\n",
    "    all_onsets=df2.all_onsets.values\n",
    "\n",
    "    # if p==0:\n",
    "    #    embeddings=emb\n",
    "    # else:\n",
    "    #     embeddings=np.concatenate((embeddings,emb),axis=0)\n",
    "\n",
    "    for k in range(len(all_onsets)): \n",
    "\n",
    "        # print(k)\n",
    "    \n",
    "        conv_name=df.conversation_name.values[k]\n",
    "        x=all_onsets[k]\n",
    "\n",
    "        if len(x)>= lag_number and x[-1] < len(ecogs[:,0]):\n",
    "        \n",
    "            a1= get_elec_data(subject, df, ecogs, conv_name, x, lags, elec_id, lag_number,taking_words)\n",
    "            electrode_data[p,:,:]=a1\n",
    "            embeddings.append(emb[k])\n",
    "            p=p+1\n",
    "\n",
    "    # p=p+1\n",
    "electrode_data=electrode_data[:p,:,:]\n",
    "embeddings=np.asarray(embeddings)\n",
    "pca = PCA(n_components=emb_dim)\n",
    "embeddings=pca.fit_transform(embeddings)\n",
    "embeddings = np.expand_dims(embeddings, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ee08e0a-9169-48a8-b9d1-b54fe4e0a06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4394, 1, 50)\n",
      "(4394, 40, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(embeddings))\n",
    "print(np.shape(electrode_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33036e96-53f2-4662-af2a-703b1b41a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_data=electrode_data[:,electrode,:]\n",
    "elec_data = np.expand_dims(elec_data, axis=1)\n",
    "X_train=torch.from_numpy(embeddings[:train_num,:,:])\n",
    "y_train=torch.from_numpy(elec_data[:train_num,:,:])\n",
    "\n",
    "X_test=torch.from_numpy(embeddings[train_num:,:,:])\n",
    "y_test=torch.from_numpy(elec_data[train_num:,:,:])\n",
    "\n",
    "trainset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "testset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce098945-8a04-4b3b-a1c3-ed1f1b94f9cd",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "211dca19-fc79-4333-b5cc-0c7297168b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class flex_encoding(nn.Module):\n",
    "    def __init__(self, emb_dim, num_lags):\n",
    "        super().__init__()\n",
    "        # the 3 Linear layers of the MLP\n",
    "        self.fc1 = nn.Linear(emb_dim, 1)\n",
    "        self.fc2 = nn.Linear(emb_dim, num_lags)\n",
    "        # self.fc3 = nn.Linear(n_hidden, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y_pred= (self.fc1(x))\n",
    "        y_pred=torch.repeat_interleave(y_pred, 3, dim=-1)\n",
    "        # v= nn.Softmax(dim=1)(self.fc2(x))\n",
    "        \n",
    "    \n",
    "        return y_pred\n",
    "        # return y_pred,v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37138a31-ce95-4ba0-bd0c-fa79435821e8",
   "metadata": {},
   "source": [
    "## custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "befbf358-bb5f-4194-b922-6fcd4f42dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bdot(a, b):  ##\n",
    "    B = a.shape[0]\n",
    "    S = a.shape[2]\n",
    "    return torch.bmm(a.view(B, 1, S), b.view(B, S, 1)).reshape(-1)\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, v, targets):\n",
    "        mse_error = torch.square(targets-y_pred)\n",
    "        loss=bdot(v, mse_error)\n",
    "        return loss.mean()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "36be65bb-99a1-45ac-861d-f65e073c49ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=flex_encoding(emb_dim, num_lags)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# loss_fn = CustomLoss()\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69343781-887d-4781-852e-c147ce3e1311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 3, 1, 1]              51\n",
      "================================================================\n",
      "Total params: 51\n",
      "Trainable params: 51\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(summary(model, (3, 1, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d30736d-2371-4d49-beb1-c2d45d19cb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.zeros((3,1,1))\n",
    "x=torch.from_numpy(x)\n",
    "# x = torch.tensor([1, 2, 3])\n",
    "x=torch.repeat_interleave(x, 3, dim=-1)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47b849ab-dc69-4db6-9570-e09a12e98a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(trainloader):\n",
    "        # Every data instance is an input + label pair\n",
    "        x,y=batch\n",
    "    \n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Make predictions for this batch\n",
    "        x = x.to(torch.float32)\n",
    "        x = x.to(device)\n",
    "        y = y.to(torch.float32)\n",
    "        y = y.to(device)\n",
    "        output1=model(x)\n",
    "\n",
    "        print(output1.size())\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1e1301d-7054-45a8-8f43-99d5bd8e8450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c56879c-feb4-4a12-923b-aa5e4488dc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for batch_idx, batch in enumerate(trainloader):\n",
    "        # Every data instance is an input + label pair\n",
    "        x,y=batch\n",
    "    \n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Make predictions for this batch\n",
    "        x = x.to(torch.float32)\n",
    "        x = x.to(device)\n",
    "        y = y.to(torch.float32)\n",
    "        y = y.to(device)\n",
    "        [output1,output2]=model(x)\n",
    "    \n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(output1,output2, y)\n",
    "        loss.backward()\n",
    "    \n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    last_loss = running_loss / (batch_idx+1) # loss per batch\n",
    "    print('epoch {} loss: {}'.format(epoch_index, last_loss))\n",
    "            \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f60e1b-45f1-48be-acee-760a35f67101",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b35c62a-ad41-456a-b468-10da1ac09232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m train_one_epoch(epoch)\n\u001b[1;32m      9\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode, disabling dropout and using population\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# statistics for batch normalization.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[48], line 20\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index)\u001b[0m\n\u001b[1;32m     18\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 20\u001b[0m [output1,output2]\u001b[38;5;241m=\u001b[39mmodel(x)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output1,output2, y)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(testloader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs = vinputs.to(torch.float32)\n",
    "            vinputs = vinputs.to(device)\n",
    "            vlabels = vlabels.to(torch.float32)\n",
    "            vlabels = vlabels.to(device)\n",
    "            [output1,output2]=model(vinputs)\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(output1,output2, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803d801-2d60-4c26-a33d-e89818f0c9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c5f0c-67a7-40c1-b1d8-31ea8994acdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cd02389-8ef6-4cf0-a8cc-d57e4d23b2a5",
   "metadata": {},
   "source": [
    "## draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "991713e3-12bd-4eed-8101-3a403ee20e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2207, 0.2205, 0.0548],\n",
       "         [0.4471, 0.7019, 0.2011]]),\n",
       " tensor([[0.3700, 0.6771, 0.2727],\n",
       "         [0.3288, 0.3360, 0.5386]]),\n",
       " tensor([[0.5545, 0.7131, 0.0221],\n",
       "         [0.6565, 0.6868, 0.2227]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.rand(2,9).chunk(3, dim=-1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15346c3b-ba6e-4c96-af84-cf97d047b731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor([1,0,3])\n",
    "\n",
    "nanned_signal = torch.where(\n",
    "                        x == 0, torch.tensor(float(\"nan\")), x\n",
    "                    )\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63723ada-6776-4448-8918-c04f3e0d9ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., nan, 3.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nanned_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f60884d-871c-417a-920d-8efc61813a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_patch_size=1\n",
    "tube_mask_ratio=0.5\n",
    "num_patches=1*8*8*40\n",
    "num_frames=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8833fa2a-024d-40dc-b118-f681435695b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9bf3294-b4e3-46ff-972d-8711799d4398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tube_mask = (\n",
    "        torch.zeros(num_patches // (num_frames // frame_patch_size))\n",
    "        .to(device)\n",
    "        .to(torch.bool)\n",
    "    )\n",
    "tube_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b0a8e43-ccad-4032-85fc-b4e9e9f92b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal=torch.rand(2,5, 40,1,8,8)\n",
    "padding_mask = ~torch.isnan(signal).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "674f9c3b-0957-41f2-bfc1-72b96c66ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45640ae7-e138-4a6f-a323-26783d6bd3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2560, 5])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask=rearrange(padding_mask, 'b c (f pf) (d pd) (h ph) (w pw) -> b f d h w (pd ph pw pf c)',pd=1,\n",
    "ph=1,\n",
    "pw=1,\n",
    "pf=1)\n",
    "padding_mask = rearrange(padding_mask, \"b ... d -> b (...) d\")\n",
    "padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e2bf7e7-88ee-4da3-bf84-d3d361f0d1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask = torch.all(padding_mask, dim=0)\n",
    "padding_mask = torch.all(padding_mask, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3dca88d2-8c26-447d-9446-2888ca0fc6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2560])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba38367f-2d76-47a9-87eb-c355aa069372",
   "metadata": {},
   "outputs": [],
   "source": [
    "chn_idx = torch.nonzero(padding_mask[: len(tube_mask)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "932ec66b-dd6d-4137-86e3-35025929260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_idx_candidates = chn_idx[torch.randperm(len(chn_idx))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "357a4c0e-1c2d-4c09-9f38-e259a43d030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tube_idx = mask_idx_candidates[\n",
    "        : int(\n",
    "            num_patches\n",
    "            // (num_frames // frame_patch_size)\n",
    "            * (1 - tube_mask_ratio)\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ef4075e-90a7-4507-8ff8-c6221c37be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tube_mask[tube_idx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd5251c1-95f4-45a2-b489-490c1a61a34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  True, False, False,  True, False,  True, False,\n",
       "        False,  True,  True,  True,  True, False,  True,  True, False, False,\n",
       "         True,  True,  True,  True,  True,  True, False, False,  True,  True,\n",
       "         True, False,  True,  True,  True, False, False, False, False, False,\n",
       "         True, False,  True, False,  True,  True,  True, False,  True,  True,\n",
       "        False,  True, False,  True, False,  True, False, False, False, False,\n",
       "        False, False,  True, False], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tube_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abf3adab-42e1-4963-8155-0f1e598d458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tube_mask = tube_mask.tile(num_frames // frame_patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4c9d69f-9832-4528-ab44-00a4f8c375f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=[]\n",
    "actual=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "192ead75-b836-4560-b4d4-8cc2ca22cc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0985, 0.1855, 0.4723],\n",
      "        [0.0242, 0.0191, 0.0627]])\n",
      "tensor([[0.0772, 0.7507, 0.0346],\n",
      "        [0.1365, 0.9196, 0.6222]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(2,3)\n",
    "print(a)\n",
    "\n",
    "b=torch.rand(2,3)\n",
    "print(b)\n",
    "\n",
    "a1=torch.min(a,dim=-1)\n",
    "predicted.append((a1.values))\n",
    "\n",
    "for i in range(b.shape[0]):\n",
    "    \n",
    "    actual.append(b[i, a1.indices[i]] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6c3839e-a220-4e42-975f-7b2e14b8a14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.3031, 0.3336]), tensor([0.0985, 0.0191])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "736b66f9-dfa5-48b9-8632-04e4313d7d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0020), tensor(0.6603)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6d3a29ea-b4d7-461a-948c-74d803522dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30310047, 0.33358306, 0.09848493, 0.01910239], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = [t.cpu().numpy() for t in predicted]\n",
    "\n",
    "predicted1=np.asarray(b1)\n",
    "\n",
    "predicted1=predicted1.flatten()\n",
    "predicted1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b85a228c-0080-4c47-a9ec-00c87fef85c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.6246), tensor(0.6726), tensor(0.0772), tensor(0.9196)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ccf43-eb55-4cb2-9b64-54d67e6f0410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-arnab [~/.conda/envs/torch-arnab/]",
   "language": "python",
   "name": "conda_torch-arnab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
